{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>\n",
    "<font size='7' style=\"color:#0D47A1\">Convolutional Neural Network (CNN)<b> </b> </font>\n",
    "</center>\n",
    "\n",
    "<hr style= \"height:3px;\">\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6' style=\"color:#00A6D6\">  <b>What are you expected to learn in today's class</b> </font>\n",
    "\n",
    "#### [Basic]\n",
    "#### * CNN Properties: Why CNN for image? Why it is (sometimes ) better than DNN?\n",
    "#### * How does CNN work? \n",
    "#### * Some well-known CNN architecture\n",
    "\n",
    "#### [Advanced]\n",
    "#### * Use tensorflow to build CNN\n",
    "#### * Computer Vision example: cifar-10\n",
    "#### * Take Home Message for training CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Question: What if we use ordinary Feedforward DNN with image?</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Picachu&Feedforward&noref.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Transform image to one dim vector.\n",
    "\n",
    "#### Too much weight >> time consuming training process\n",
    "\n",
    "#### Question: Does the top left grid relate to bottom right grid?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Characteristic of Image</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Graph_composition.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Can you recognize these images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Patterns_powerful.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Why CNN for Image?</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5' style=\"color:#4CAF50\">  <b>[Property 1_What] : Some patterns are much smaller than the whole image</b></font>\n",
    "\n",
    "\n",
    "\n",
    "### A neuron does not have to see the whole image to discover the pattern\n",
    "### Connecting to small region with less parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CNNProperty1_what.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size='5' style=\"color:#4CAF50\">  <b>[Property 2_Where] : The same patterns appear in different regions</b></font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <img src=\"CNNProperty2_where.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CNNProperty2_share_parameter.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5' style=\"color:#4CAF50\">  <b>[Property 3_Size] : Sampling the pixels will not change the object</b></font>\n",
    "\n",
    "\n",
    "### We can subsample the pixels to make the image smaller\n",
    "### >>> Less parameters for the network to process the image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CNNProperty3_subsampling.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<font size='5' style=\"color:#4CAF50\">  <b>Why use CNN for image?</b></font>\n",
    "\n",
    "    1. What   : Some patterns are much smaller than the whole image\n",
    "    2. Where : The same patterns appear in different regions\n",
    "    3. Size     : Sampling the pixels will not change the object "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>How does Convolution work in Computer Vision (CV)?</b> </font>\n",
    "0. Convolution, Filter and Feature Map\n",
    "1. Filter Size \n",
    "2. Padding\n",
    "3. Stride"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size='5' style=\"color:#4CAF50\">  <b>Convolution, Filters and Feature Map</b></font>\n",
    " \n",
    "### Adding each pixel and its local neighbors which are weighted by a filter (kernel)\n",
    "### Use \"Filter\" to perform Convolution process on each pixels to generate Feature Map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"HowCNNwork_1.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"HowCNNwork_2_changing.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML('<img src=\"HowCNNwork_2_changing.gif\">')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size='5' style=\"color:#4CAF50\">  <b>How does filter work on the graph?</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CNN_mouse_filter.png\" alt=\"Drawing\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CNN_mouse_1.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CNN_mouse_2.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>A classical CNN architecture</b> </font>\n",
    "\n",
    "### [CNN]\n",
    "### * Convolutional Layer\n",
    "### * Activation Layer\n",
    "### * Pooling Layer\n",
    "### [DNN]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CNN_architecture.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Convolutional Layer and its Properties</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5' style=\"color:#4CAF50\">  <b>Convolution Property 1: Convolution times</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Convolution_property_1.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The more you execute convolution, the smaller the image become."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<font size='5' style=\"color:#4CAF50\">  <b>Convolution Property 2: Filter size</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"Convolution_property_2_filtersizeS.png\" alt=\"Drawing\" style=\"width: 750px;\"/>\n",
    "\n",
    "\n",
    "<img src=\"Convolution_property_2_filtersizeL.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Filter size is one of hyperparameters you generally need to tune. \n",
    "### Some useful observations: preferred to choose smaller filters, but have greater number of those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size='5' style=\"color:#4CAF50\">  <b>Convolution Property 3: Filter number</b></font>\n",
    "\n",
    "### Different filters learn different patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"HowFilterwork_changing_resize25.gif\">"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "# HTML('<img src=\"HowFilterwork_changing.gif\">' )\n",
    "# HTML('<img src=\"HowFilterwork_changing_resize2.gif\">' )\n",
    "HTML('<img src=\"HowFilterwork_changing_resize25.gif\">' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All the numbers in the filter are the network parameters to be learned!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5' style=\"color:#4CAF50\">  <b>Convolution property 4: padding</b></font>\n",
    "\n",
    "\n",
    "#### padding  ='valid' means without padding\n",
    "#### padding  ='same' means perform padding (default value is zero)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Convolution_property_3_zeropadding_comparison.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add additional zeros at the boarder of the image. \n",
    "### >> Padding can preserve the spatial dimension of the images.\n",
    "### >> Padding can preserve more edge information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size='5' style=\"color:#4CAF50\">  <b>Convolution property 5: stride  >> how far the filter is moving</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Convolution_property_4_Stride.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5' style=\"color:#4CAF50\">  <b>Review of Properties of Convolution Layers</b></font> \n",
    "\n",
    "    1. Convolution times  >> make the image smaller\n",
    "    2. Filter Size        >> adjust output image's size\n",
    "    3. Padding            >> used to keep the image's size\n",
    "    4. Stride             >> make the image smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Activation Layer</b> </font>\n",
    "\n",
    "### Timing: Immediately after each conv layer \n",
    "### Purpose: nonlinearity\n",
    "### Example:  ReLU (Best choice), tanh and sigmoid\n",
    "### Rectified Linear Unit (ReLU): f(x) = max(0, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Activation_Layer.png\" alt=\"Drawing\" style=\"width: 850px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After each conv layer, it is convention to apply a nonlinear layer (or activation layer) immediately afterward.The purpose of this layer is to introduce nonlinearity to a system that basically has just been computing linear operations during the conv layers (just element wise multiplications and summations).In the past, nonlinear functions like tanh and sigmoid were used, but researchers found out that ReLU layers work far better because the network is able to train a lot faster (because of the computational efficiency) without making a significant difference to the accuracy. It also helps to alleviate the vanishing gradient problem, which is the issue where the lower layers of the network train very slowly because the gradient decreases exponentially through the layers (Explaining this might be out of the scope of this post, but see https://en.wikipedia.org/wiki/Vanishing_gradient_problem and https://www.quora.com/What-is-the-vanishing-gradient-problem for good descriptions). The ReLU layer applies the function f(x) = max(0, x) to all of the values in the input volume. In basic terms, this layer just changes all the negative activations to 0.This layer increases the nonlinear properties of the model and the overall network without affecting the receptive fields of the conv layer.\n",
    "\n",
    "Paper by the great Geoffrey Hinton (aka the father of deep learning). http://www.cs.toronto.edu/~fritz/absps/reluICML.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Pooling Layer</b> </font>\n",
    "### Why do we need pooling layers?\n",
    "* Reduce the number of weights\n",
    "* Prevent overfitting\n",
    "\n",
    "### Max pooling\n",
    "* Consider the existence of patterns in each region\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Pooling_Layer.png\" alt=\"Drawing\" style=\"width: 800px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='6' style=\"color:#00A6D6\">  <b>How to connect CNN with DNN</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Flatten_cnn_dnn.png\" alt=\"Drawing\" style=\"width: 850px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CNN_architecture_layer_car.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"CNN_architecture_classify_car.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>ImageNet</b> </font>\n",
    "\n",
    "* ~14 million labeled images, 20k classes\n",
    "* Images gathered from Internet\n",
    "* Human labels via Amazon MTurk\n",
    "\n",
    "<br>\n",
    "<font size='5' style=\"color:#4CAF50\">  <b>ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</b></font>\n",
    "\n",
    "### Classes : 1000 image categories\n",
    "### Amount :  1200k training 100k testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ImageNet_hard.png\" alt=\"Drawing\" style=\"width: 950px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Modern CNN Architecture</b> </font>\n",
    "\n",
    "### 1. (1988) LeNet-5 {7} \n",
    "### 2. (2012) AlexNet {8}\n",
    "### 3. (2014) VGG {16/19}\n",
    "### 4. (2014) GoogLeNet {22}\n",
    "### 5. (2015) ResNet {152}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size='5' style=\"color:#4CAF50\">  <b>LeNet-5 { 7 Layers }</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Architecture_LeNet.png\" alt=\"Drawing\" style=\"width: 950px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Famous : 5x5 Conv\n",
    "### Simple Architecture : Conv-Pool-Conv-Pool-FC-FC (has everything Conv / Nonlinearity ReLU / Pooling / Classification )\n",
    "### Limitation :  Computing power was bad in 1988...\n",
    "### Useful : several banks to recognise hand-written numbers on checks digitized in 32x32 pixel greyscale images\n",
    "### Demo : http://scs.ryerson.ca/~aharley/vis/conv/flat.html\n",
    "### Ref : https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ILSVRC_2012AlexNet.png\" alt=\"Drawing\" style=\"width: 950px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size='5' style=\"color:#4CAF50\">  <b>AlexNet { 8 Layers }</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Architecture_AlexNet_NN.png\" alt=\"Drawing\" style=\"width: 950px;\"/>\n",
    "<img src=\"Architecture_AlexNet_Layer.png\" alt=\"Drawing\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Famous\n",
    "### 1. Data augmentation (ex. flip, crops, jittering)\n",
    "### 2. First use ReLU nonlinearity\n",
    "### 3. Dropout Layer\n",
    "### 4. GPU implementation (50x speedup over CPU), trained on two GPUs for a week"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"dropout_layer_comparison.png\" alt=\"Drawing\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size='5' style=\"color:#4CAF50\">  <b>Dropout Layer</b></font>\n",
    "\n",
    "### Purpose : Regularization \n",
    "### Side Effect : Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ILSVRC_2014VGG&GoogLeNet.png\" alt=\"Drawing\" style=\"width: 950px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size='5' style=\"color:#4CAF50\">  <b>VGG 16 / VGG 19</b></font>\n",
    "\n",
    "### Famous: Use lots of 3 x 3 Conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Architecture_VGG1619_Layer.png\" alt=\"Drawing\" style=\"width: 400px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"VGG_Memory&parameter.png\" alt=\"Drawing\" style=\"width: 900px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size='5' style=\"color:#4CAF50\">  <b>GoogLeNet</b></font>\n",
    "\n",
    "### Famous : \n",
    "### 1.  22 layers\n",
    "### 2.  Efficient “Inception” module\n",
    "### 3.  No FC layers\n",
    "### 4.  Only 5 million parameters! (12x less than AlexNet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"GoogLeNet_whole.png\" alt=\"Drawing\" style=\"width: 1000px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"GoogLeNet_inception_naive&1x1.png\" alt=\"Drawing\" style=\"width: 950px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"GoogLeNet_stacked_Inception.png\" alt=\"Drawing\" style=\"width: 950px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# <img src=\"GoogLeNet_inception_concat.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<br>\n",
    "<font size='5' style=\"color:#4CAF50\">  <b>How does 1 x 1 Convolution work?</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"con1x1_1.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"con1x1_2.png\" alt=\"Drawing\" style=\"width: 750px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size='5' style=\"color:#4CAF50\">  <b>ResNet</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ILSVRC_2015ResNet.png\" alt=\"Drawing\" style=\"width: 850px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Complexity</b> </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ILSVRC_complexity_GoogLeNet.png\" alt=\"Drawing\" style=\"width: 950px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ILSVRC_complexity_VGG.png\" alt=\"Drawing\" style=\"width: 950px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"ILSVRC_complexity_AlexNet.png\" alt=\"Drawing\" style=\"width: 950px;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<font size='5' style=\"color:#4CAF50\">  <b>Review of CNN Property and CNN Architecture</b></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Review_CNNProperty_Architecture.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Take Home Message for training CNN</b> </font>\n",
    "### Filter size : 3x3 (Recommendation)\n",
    "### Filter numbers : increasing (64 > 128 > 256)\n",
    "### Data-augmentation : Increase Dataset\n",
    "### Maxpooling : Not too much, otherwise you will loss too many information\n",
    "### Overfitting techniques : Dropout , L2 regularization , Earlystop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Reference</b> </font>\n",
    "* http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture9.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<font size='6' style=\"color:#00A6D6\">  <b>Figure Reference</b> </font>\n",
    "* http://www.sumiaozhijia.com/touxiang/471.html\n",
    "* http://122311.com/tag/su-miao/2.html\n",
    "* ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
